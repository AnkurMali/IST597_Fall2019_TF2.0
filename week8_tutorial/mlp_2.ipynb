{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAI0A-Xhd-em",
        "colab_type": "text"
      },
      "source": [
        "# IST597:- Multi-layer Perceptron\n",
        "#Week 8 tutorial\n",
        "Building your first MLP in eager\n",
        "Author :- aam35"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd4TZRaCd9dP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a6b9240f-bafe-4641-8994-38d8de90404a"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Author:-aam35\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import time\n",
        "tf.enable_eager_execution()\n",
        "tf.executing_eagerly()\n",
        "\n",
        "# random seed to get the consistent result\n",
        "tf.random.set_random_seed(1234)\n",
        "\n",
        "data = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
        "\n",
        "\n",
        "minibatch_size = 32\n",
        "learning_rate = 0.01"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--oJxQEMxrE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Simple MLP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWiECZfAfYw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## model 1\n",
        "size_input = 784 # MNIST data input (img shape: 28*28)\n",
        "size_hidden = 256\n",
        "size_output = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden: int, size of hidden layer\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "        \"\"\"\n",
        "        self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "        size_input, size_hidden, size_output, device\n",
        "    \n",
        "        # Initialize weights between input layer and hidden layer\n",
        "        self.W1 = tf.Variable(tf.random_normal([self.size_input, self.size_hidden],stddev=0.1),name=\"W1\")\n",
        "        # Initialize biases for hidden layer\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden]), name = \"b1\")\n",
        "        # Initialize weights between hidden layer and output layer\n",
        "        self.W2 = tf.Variable(tf.random_normal([self.size_hidden, self.size_output],stddev=0.1),name=\"W2\")\n",
        "        # Initialize biases for output layer\n",
        "        self.b2 = tf.Variable(tf.random_normal([1, self.size_output]),name=\"b2\")\n",
        "    \n",
        "\n",
        "        \n",
        "        # Define variables to be updated during backpropagation\n",
        "        self.variables = [self.W1, self.b1,self.W2, self.b2]\n",
        "        \n",
        "    \n",
        "    # prediction\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward pass\n",
        "        X: Tensor, inputs\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "      \n",
        "        return self.y\n",
        "    \n",
        "    ## loss function\n",
        "    def loss(self, y_pred, y_true):\n",
        "        '''\n",
        "        y_pred - Tensor of shape (batch_size, size_output)\n",
        "        y_true - Tensor of shape (batch_size, size_output)\n",
        "        '''\n",
        "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
        "        \n",
        "  \n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        backward pass\n",
        "        \"\"\"\n",
        "        # optimizer\n",
        "        # Test with SGD,Adam, RMSProp\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        optimizer.apply_gradients(zip(grads, self.variables),\n",
        "                              global_step=tf.train.get_or_create_global_step())\n",
        "        \n",
        "        \n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to obtain output tensor during forward pass\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        #Remember to normalize your dataset before moving forward\n",
        "        # Compute values in hidden layer\n",
        "        what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        hhat = tf.nn.relu(what)\n",
        "        # Compute output\n",
        "        output = tf.matmul(hhat, self.W2) + self.b2\n",
        "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "        #Second add tf.Softmax(output) and then return this variable\n",
        "        #print(output)\n",
        "        return (output)\n",
        "        #return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MS4cW88hwbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## model 2\n",
        "size_input = 784 # MNIST data input (img shape: 28*28)\n",
        "size_hidden = 256\n",
        "size_output = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "\n",
        "# Define class to build mlp model\n",
        "class MLP_2(object):\n",
        "    def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden: int, size of hidden layer\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "        \"\"\"\n",
        "        self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "        size_input, size_hidden, size_output, device\n",
        "    \n",
        "        # Initialize weights between input layer and hidden layer\n",
        "        self.W1 = tf.Variable(tf.random_normal([self.size_input, self.size_hidden],stddev=0.1),name=\"W_1\")\n",
        "        # Initialize biases for hidden layer\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden]), name = \"b_1\")\n",
        "        # Initialize weights between hidden layer and output layer\n",
        "        self.W2 = tf.Variable(tf.random_normal([self.size_hidden, self.size_output],stddev=0.1),name=\"W_2\")\n",
        "        # Initialize biases for output layer\n",
        "        self.b2 = tf.Variable(tf.random_normal([1, self.size_output]),name=\"b_2\")\n",
        "    \n",
        "\n",
        "        \n",
        "        # Define variables to be updated during backpropagation\n",
        "        self.variables = [self.W1, self.b1,self.W2, self.b2]\n",
        "        \n",
        "    \n",
        "    # prediction\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward pass\n",
        "        X: Tensor, inputs\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "      \n",
        "        return self.y\n",
        "    \n",
        "    ## loss function\n",
        "    def loss(self, y_pred, y_true):\n",
        "        '''\n",
        "        y_pred - Tensor of shape (batch_size, size_output)\n",
        "        y_true - Tensor of shape (batch_size, size_output)\n",
        "        '''\n",
        "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
        "        \n",
        "  \n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        backward pass\n",
        "        \"\"\"\n",
        "        # optimizer\n",
        "        # Test with SGD,Adam, RMSProp\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        optimizer.apply_gradients(zip(grads, self.variables),\n",
        "                              global_step=tf.train.get_or_create_global_step())\n",
        "        \n",
        "        \n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to obtain output tensor during forward pass\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        #Remember to normalize your dataset before moving forward\n",
        "        # Compute values in hidden layer\n",
        "        what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        hhat = tf.nn.relu(what)\n",
        "        hhat_tilda = tf.compat.v1.nn.dropout(hhat,rate=0.2)\n",
        "        # Compute output\n",
        "        output = tf.matmul(hhat_tilda, self.W2) + self.b2\n",
        "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "        #Second add tf.Softmax(output) and then return this variable\n",
        "        #print(output)\n",
        "        return (output)\n",
        "        #return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1rJr1fCxjX7",
        "colab_type": "text"
      },
      "source": [
        "MLP with regularizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5CxXHIvjR6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## model 3\n",
        "size_input = 784 # MNIST data input (img shape: 28*28)\n",
        "size_hidden = 256\n",
        "size_output = 10 # MNIST total classes (0-9 digits)\n",
        "beta = 0.01\n",
        "\n",
        "\n",
        "# Define class to build mlp model\n",
        "class MLP_3(object):\n",
        "    def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden: int, size of hidden layer\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "        \"\"\"\n",
        "        self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "        size_input, size_hidden, size_output, device\n",
        "    \n",
        "        # Initialize weights between input layer and hidden layer\n",
        "        self.W1 = tf.Variable(tf.random_normal([self.size_input, self.size_hidden],stddev=0.1),name=\"W_13\")\n",
        "        # Initialize biases for hidden layer\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden]), name = \"b_13\")\n",
        "        # Initialize weights between hidden layer and output layer\n",
        "        self.W2 = tf.Variable(tf.random_normal([self.size_hidden, self.size_output],stddev=0.1),name=\"W_23\")\n",
        "        # Initialize biases for output layer\n",
        "        self.b2 = tf.Variable(tf.random_normal([1, self.size_output]),name=\"b_23\")\n",
        "    \n",
        "\n",
        "        \n",
        "        # Define variables to be updated during backpropagation\n",
        "        self.variables = [self.W1, self.b1,self.W2, self.b2]\n",
        "        \n",
        "    \n",
        "    # prediction\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward pass\n",
        "        X: Tensor, inputs\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "      \n",
        "        return self.y\n",
        "    \n",
        "    ## loss function\n",
        "    def loss(self, y_pred, y_true):\n",
        "        '''\n",
        "        y_pred - Tensor of shape (batch_size, size_output)\n",
        "        y_true - Tensor of shape (batch_size, size_output)\n",
        "        '''\n",
        "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
        "        regularizers = tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.W2)\n",
        "        Reg_loss = tf.reduce_mean(loss + beta * regularizers)\n",
        "        return Reg_loss\n",
        "        \n",
        "  \n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        backward pass\n",
        "        \"\"\"\n",
        "        # optimizer\n",
        "        # Test with SGD,Adam, RMSProp\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        optimizer.apply_gradients(zip(grads, self.variables),\n",
        "                              global_step=tf.train.get_or_create_global_step())\n",
        "        \n",
        "        \n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to obtain output tensor during forward pass\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        #Remember to normalize your dataset before moving forward\n",
        "        # Compute values in hidden layer\n",
        "        what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        hhat = tf.nn.relu(what)\n",
        "        # Compute output\n",
        "        output = tf.matmul(hhat, self.W2) + self.b2\n",
        "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "        #Second add tf.Softmax(output) and then return this variable\n",
        "        #print(output)\n",
        "        return (output)\n",
        "        #return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrAhBTjCw1bw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "create accuracy function which takes prediction and targets as input\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1O8vTLemp9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_function(yhat,true_y):\n",
        "  correct_prediction = tf.equal(tf.argmax(yhat, 1), tf.argmax(true_y, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  return accuracy\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9ll9yEMklDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "ed28a4b9-f441-4bfc-8bc2-868d5826e48a"
      },
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "num_epochs = 8\n",
        "\n",
        "time_start = time.time()\n",
        "num_train = 55000\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
        "           .shuffle(buffer_size=1000)\\\n",
        "           .batch(batch_size=minibatch_size)\n",
        "        loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "        for inputs, outputs in train_ds:\n",
        "            preds = mlp_on_cpu.forward(inputs)\n",
        "            loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "            mlp_on_cpu.backward(inputs, outputs)\n",
        "        print('Number of Epoch = {} - loss:= {:.4f}'.format(epoch + 1, loss_total.numpy() / num_train))\n",
        "        preds = mlp_on_cpu.compute_output(data.train.images)\n",
        "        accuracy_train = accuracy_function(preds,data.train.labels)\n",
        "        accuracy_train = accuracy_train * 100\n",
        "        print (\"Training Accuracy = {}\".format(accuracy_train.numpy()))\n",
        "        \n",
        "        preds_val = mlp_on_cpu.compute_output(data.validation.images)\n",
        "        accuracy_val = accuracy_function(preds_val,data.validation.labels)\n",
        "        accuracy_val = accuracy_val * 100\n",
        "        print (\"Validation Accuracy = {}\".format(accuracy_val.numpy()))\n",
        " \n",
        "    \n",
        "# test accuracy\n",
        "preds_test = mlp_on_cpu.compute_output(data.test.images)\n",
        "accuracy_test = accuracy_function(preds_test,data.test.labels)\n",
        "# To keep sizes compatible with model\n",
        "accuracy_test = accuracy_test * 100\n",
        "print (\"Test Accuracy = {}\".format(accuracy_test.numpy()))\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Epoch = 1 - loss:= 0.0193\n",
            "Training Accuracy = 89.207275390625\n",
            "Validation Accuracy = 89.5\n",
            "Number of Epoch = 2 - loss:= 0.0105\n",
            "Training Accuracy = 91.04545593261719\n",
            "Validation Accuracy = 91.23999786376953\n",
            "Number of Epoch = 3 - loss:= 0.0090\n",
            "Training Accuracy = 92.18909454345703\n",
            "Validation Accuracy = 92.47999572753906\n",
            "Number of Epoch = 4 - loss:= 0.0080\n",
            "Training Accuracy = 92.90545654296875\n",
            "Validation Accuracy = 93.13999938964844\n",
            "Number of Epoch = 5 - loss:= 0.0073\n",
            "Training Accuracy = 93.53636169433594\n",
            "Validation Accuracy = 93.68000030517578\n",
            "Number of Epoch = 6 - loss:= 0.0067\n",
            "Training Accuracy = 94.18000030517578\n",
            "Validation Accuracy = 94.27999877929688\n",
            "Number of Epoch = 7 - loss:= 0.0062\n",
            "Training Accuracy = 94.54181671142578\n",
            "Validation Accuracy = 94.54000091552734\n",
            "Number of Epoch = 8 - loss:= 0.0058\n",
            "Training Accuracy = 94.96726989746094\n",
            "Validation Accuracy = 94.94000244140625\n",
            "Test Accuracy = 94.66000366210938\n",
            "\n",
            "Total time taken (in seconds): 136.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL11Y1-XvYxP",
        "colab_type": "text"
      },
      "source": [
        "For one layer feed-forward we can easily reach in high 90's\n",
        "# Things to try\n",
        "* Increase the depth or layers of this model and analyze the change in convergence\n",
        "* Change the optimizer , play with learning rate , try various activation functions.\n",
        "* You can also use various initialization method[We have used random_init, play with xavier or orthogonal]\n",
        "* Tweak the network, for better understanding how hyper-parameters plays a vital role in achieving state of the art."
      ]
    }
  ]
}