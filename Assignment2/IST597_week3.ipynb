{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST597_week3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Fall2019_TF2.0/blob/master/IST597_week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7r1fJQniHj7",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial IST597:- Intro to Eager Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwZoGf85dnU",
        "colab_type": "text"
      },
      "source": [
        "# Enabling Eager Execution \n",
        "In version 2.0 eager execution is set TRUE by default.For all other versions $<1.7$ enable using tf.enable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlnQG8hC-uCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fq5Gb135Z5f",
        "colab_type": "text"
      },
      "source": [
        "Check if eager execution is enabled or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS7er1hy-7yO",
        "colab_type": "code",
        "outputId": "10afc19a-a20c-4176-a2c5-3ec09a389be5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.executing_eagerly()\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC6X5Y844_E-",
        "colab_type": "text"
      },
      "source": [
        "# Executing tf Ops Eagerly \n",
        "More pythonic : Since by perfoming operations we can see the output directly.\n",
        "No Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmwZJKlA_B15",
        "colab_type": "code",
        "outputId": "b084401e-52ad-4c7b-a536-e24bc3dc2884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = [[2.]]\n",
        "m = tf.square(x)\n",
        "print(m)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGLoIPrQ6ZYT",
        "colab_type": "text"
      },
      "source": [
        "Also can call `.numpy` to retrieve the results of the tensor as a numpy array.[Useful for people who are familiar with pytorch or numpy]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FGFGbZq6fRo",
        "colab_type": "code",
        "outputId": "6b58fea0-813d-45de-b6ea-54c437e4789c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "m.numpy()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Xlu7in6m22",
        "colab_type": "text"
      },
      "source": [
        "compute an operation including two tensors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4tKJJ90_QMM",
        "colab_type": "code",
        "outputId": "74b73158-045d-41c9-d44e-9b2ce9daee66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "a = tf.constant([[1, 2],\n",
        "                 [3, 4]])\n",
        "\n",
        "b = tf.constant([[2, 1],\n",
        "                 [3, 4]])\n",
        "\n",
        "ab = tf.matmul(a, b)\n",
        "\n",
        "print('a * b = \\n', ab.numpy())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a * b = \n",
            " [[ 8  9]\n",
            " [18 19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5qlVJygETcb",
        "colab_type": "text"
      },
      "source": [
        "# Constants and Variables [Understand the difference]\n",
        "\n",
        "\n",
        "*   `tf.constant`, creates a constant tensor populated with the values as argument. The values are immutable. \n",
        "*   `tf.Variable `, this method encapsultes a mutable tensor that can be changed later using assign. \n",
        "(From official tensorflow documentation.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayMVXFj1FZxz",
        "colab_type": "text"
      },
      "source": [
        "Create a constant tensor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2KFQKSLFNlS",
        "colab_type": "code",
        "outputId": "8c6a8616-f326-4259-dd4d-1ebb07a16e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a = tf.constant([[2,3]])\n",
        "print(a)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn8uX4t5FtHp",
        "colab_type": "text"
      },
      "source": [
        "As we discussed constant tensor is immutable so we cannot assign a new value to it. Let's see an example for this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrJqeZfgHU-j",
        "colab_type": "code",
        "outputId": "386fa725-2efe-45a8-f1c0-267db5f6b8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "  a.assign([[3,4]])\n",
        "except:\n",
        "  print('Exception raised trying to change immutable tensor ')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception raised trying to change immutable tensor \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrgbhCu8H3rm",
        "colab_type": "text"
      },
      "source": [
        "On the other hand variables are mutable and can be assigned a new value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSMIotOQFw2F",
        "colab_type": "code",
        "outputId": "915e2ae9-93f8-4268-9988-8018b33f0ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "v = tf.Variable(5.)\n",
        "\n",
        "print('previous value v =', v.numpy())\n",
        "v.assign(2.)\n",
        "print('Current value v =', v.numpy())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "previous value v = 5.0\n",
            "Current value v = 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se0MFrEwMXWo",
        "colab_type": "text"
      },
      "source": [
        "increment/decrement the value of a tensor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9M50PpdMzEn",
        "colab_type": "code",
        "outputId": "192ac492-7d2c-47c3-fa08-1ae3508510a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "v.assign(2.)\n",
        "print('value     : ', v.numpy())\n",
        "print('increment : ', tf.assign_add(v, 1).numpy())\n",
        "print('decrement : ', tf.assign_sub(v, 1).numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value     :  2.0\n",
            "increment :  3.0\n",
            "decrement :  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtGAYUUWI8bX",
        "colab_type": "text"
      },
      "source": [
        "You can return many information from a tensor variable same as numpy, like the name, type , shape and what device it executes on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wQMtx3QJBSg",
        "colab_type": "code",
        "outputId": "914abefd-e6bc-404f-a63d-7f96de4c9e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print('name  : ', v.name)\n",
        "print('type  : ', v.dtype)\n",
        "print('shape : ', v.shape)\n",
        "print('device: ', v.device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name  :  Variable:0\n",
            "type  :  <dtype: 'float32'>\n",
            "shape :  ()\n",
            "device:  /job:localhost/replica:0/task:0/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5irSm-yDN0nV",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Evaluation[Imp Concept]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y36ig_TVAAoM",
        "colab_type": "text"
      },
      "source": [
        "Gradient evaluation is very important machine learning because it is based on function optimization. You can use `tf.GradientTape()` method to record the gradient of an arbitrary function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdZmXyAi_3M3",
        "colab_type": "code",
        "outputId": "8146082e-9f7c-4ff2-caab-03290b982bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w = tf.Variable(2.0)\n",
        "\n",
        "#watch the gradient of the loss operation\n",
        "with tf.GradientTape() as tape:\n",
        "  loss = w * w\n",
        "\n",
        "grad = tape.gradient(loss, w)\n",
        "print(f'The gradient of w^2 at {w.numpy()} is {grad.numpy()}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradient of w^2 at 2.0 is 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEMEMfV8Pwvt",
        "colab_type": "text"
      },
      "source": [
        "We can also compute the gradient directly using `gradients_function`. In this example we evaluate the gradient of the sigmoid function \n",
        "\n",
        "$$f(x) = \\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "Note that \n",
        "\n",
        "$$f'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = f(x)(1-f(x)) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrw-DuoWP0A6",
        "colab_type": "code",
        "outputId": "2a2345f6-d0b5-4be7-bd20-9219c492593c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow.contrib.eager as tfe \n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + tf.exp(-x))\n",
        "\n",
        "grad_sigmoid = tfe.gradients_function(sigmoid)\n",
        "\n",
        "print('The gradient of the sigmoid function at 2.0 is ', grad_sigmoid(2.0)[0].numpy())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradient of the sigmoid function at 2.0 is  0.104993574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jew_BsZaYeVz",
        "colab_type": "text"
      },
      "source": [
        "You can also compute higher order derivatives by nesting a gradient functions. For instance, \n",
        "\n",
        "$$f(x) = \\log(x) , f'(x) = \\frac{1}{x}, f''(x) = \\frac{-1}{x^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoFFIr_AXUnm",
        "colab_type": "code",
        "outputId": "1724a49b-2d07-4d23-cc99-45203aee686b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "dx = tfe.gradients_function\n",
        "\n",
        "def log(x):\n",
        "  return tf.log(x)\n",
        "\n",
        "dx_log = dx(log)\n",
        "dx2_log = dx(dx(log))\n",
        "dx3_log = dx(dx(dx(log)))\n",
        "\n",
        "print('The first  derivative of log at x = 1 is ', dx_log(1.)[0].numpy())\n",
        "print('The second derivative of log at x = 1 is ', dx2_log(1.)[0].numpy())\n",
        "print('The third  derivative of log at x = 1 is ', dx3_log(1.)[0].numpy())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first  derivative of log at x = 1 is  1.0\n",
            "The second derivative of log at x = 1 is  -1.0\n",
            "The third  derivative of log at x = 1 is  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBVnUE6DRDFw",
        "colab_type": "text"
      },
      "source": [
        "# Custom Gradients\n",
        "\n",
        "Some times the gradient is not what we want espeically if there is a problem in numerical instabilitiy. Consider the following function and its gradient \n",
        "\n",
        "$$f(x) = \\log(1+e^x)$$\n",
        "\n",
        "The gradient is \n",
        "\n",
        "$$f'(x) = \\frac{e^x}{1+e^x}$$\n",
        "\n",
        "Note that at big values of $x$ the gradient value will blow up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "autHEivlRp9M",
        "colab_type": "code",
        "outputId": "3e63e0d8-91c0-464d-b370-1bc28055ccd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def logexp(x):\n",
        "  return tf.log(1 + tf.exp(x))\n",
        "grad_logexp = tfe.gradients_function(logexp)\n",
        "\n",
        "print('The gradient at x = 0  is ', grad_logexp(0.)[0].numpy())  \n",
        "\n",
        "print('The gradient at x = 100 is ', grad_logexp(100.)[0].numpy()) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradient at x = 0  is  0.5\n",
            "The gradient at x = 100 is  nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19m7XawUR1a",
        "colab_type": "text"
      },
      "source": [
        " We can revaluate the gradient by overriding the gradient of the function. We can recompute the gradient as \n",
        "\n",
        "$$f(x) =  \\frac{1+e^x -e^x }{1+e^x} = 1 - \\frac{1}{1 + e^{x}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmPT2S6XUJ8C",
        "colab_type": "code",
        "outputId": "ef291a72-8f07-4238-8160-a1854902e78d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "@tf.custom_gradient\n",
        "def logexp_stable(x):\n",
        "  e = tf.exp(x)\n",
        "  #dy is optional, allows computation of vector jacobian products for vectors other than the vector of ones.\n",
        "  def grad(dy):\n",
        "    return dy * (1 - 1 / (1 + e))\n",
        "  return tf.log(1 + e), grad\n",
        "\n",
        "grad_logexp_stable = tfe.gradients_function(logexp_stable)\n",
        "\n",
        "print('The gradient at x = 100 is ', grad_logexp_stable(100.)[0].numpy()) "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradient at x = 100 is  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhLnr59mdFh2",
        "colab_type": "text"
      },
      "source": [
        "# Execution Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF0MR4mPdduw",
        "colab_type": "text"
      },
      "source": [
        "`add_execution_callback` can be used to monitor the execution of operations. These functions will be called when any function is executed eagerly. In this example we record the operation names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3ybT2GPdLqU",
        "colab_type": "code",
        "outputId": "c148bcc4-b743-498b-e2dd-968393a228ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#create a callback that records the operation name \n",
        "def print_op(op_type, op_name, attrs, inputs, outputs):\n",
        "  print(op_type)\n",
        "  \n",
        "#clear previous callbacks\n",
        "tfe.clear_execution_callbacks() \n",
        "\n",
        "#add the callback \n",
        "tfe.add_execution_callback(print_op)\n",
        "\n",
        "#try runing an operation \n",
        "x = tf.pow(2.0, 3.0) - 3.0\n",
        "x = x + 5.0\n",
        "x = x / 1.75\n",
        "\n",
        "#clear the callback \n",
        "tfe.clear_execution_callbacks() "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pow\n",
            "Sub\n",
            "Add\n",
            "RealDiv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qofbAYzRO3IW",
        "colab_type": "text"
      },
      "source": [
        "# Object Oriented Metrics\n",
        "We can use `metrics` to record tensors/values and operate on them at the end. This is useful when recording the training history and you want to evaluate it at the end. Use `.result()` to evaluate the metric at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4DkG8BBO74o",
        "colab_type": "code",
        "outputId": "50a00981-4358-4233-c4c7-69757675908a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "m = tfe.metrics.Mean(\"loss\")\n",
        "\n",
        "#record the loss \n",
        "m(2)\n",
        "m(4)\n",
        "\n",
        "print('The mean loss is ', m.result().numpy())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean loss is  3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erOTyJXWbHNJ",
        "colab_type": "text"
      },
      "source": [
        "If you want to remove the recorded values, you can reinstintiate the variables "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ20YHw9bILA",
        "colab_type": "code",
        "outputId": "56d41b66-9fb5-4a0a-8307-b2712ba19be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "m.init_variables()\n",
        "print('The mean loss is ', m.result().numpy())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean loss is  nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbIHKtEEbujP",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression \n",
        "To get you to think in terms of neural architectures, we will approach the problem of estimating\n",
        "good regression models from the perspective of incremental learning.  In other words, instead of\n",
        "using the normal equations to directly solve for a closed-form solution, we will search for optima\n",
        "(particularly minima) by iteratively calculating partial derivatives (of some cost function with respect\n",
        "to parameters) and taking steps down the resultant error landscape. The ideas you will develop and\n",
        "implement in this assignment will apply to learning the parameters of more complex computation\n",
        "graphs, including those that define convolutional neural networks and recurrent neural net.\n",
        "\n",
        "\n",
        "We create a complete example of using linear regression to predict the paramters of the function \n",
        "\n",
        "$$f(x) = 3 x + 2 + noise$$\n",
        "\n",
        "Given a point $x$ we want to predict the value of $y$. We train the model on 10000 data pairs $(x,f(x))$. \n",
        "\n",
        "The model to learn is a linear model \n",
        "\n",
        "$$\\hat{y} = W x + b$$\n",
        "\n",
        "Note that, we use `tf.GradientTape` to record the gradient with respect our trainable paramters.  \n",
        "\n",
        "We use Mean Square Error(MSE) to calcuate the loss \n",
        "\n",
        "$$g = (y-\\hat{y})^2$$\n",
        "\n",
        "Other loss function which can be used for eg L1\n",
        "$$g = (y - \\hat{y}) $$\n",
        "\n",
        "\n",
        "We use Gradient Descent to update the paramters \n",
        "\n",
        "$$W = W - \\alpha  \\frac{\\partial g}{\\partial W}$$\n",
        "\n",
        "$$b = b - \\alpha  \\frac{\\partial g}{\\partial b}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7A5lcylAERT",
        "colab_type": "code",
        "outputId": "41a49472-e19d-415d-eb65-9b9d489ca337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#1000 data points \n",
        "NUM_EXAMPLES = 500\n",
        "\n",
        "#define inputs and outputs with some noise \n",
        "X = tf.random_normal([NUM_EXAMPLES])  #inputs \n",
        "noise = tf.random_normal([NUM_EXAMPLES]) #noise \n",
        "y = X * 3 + 2 + noise  #true output\n",
        "\n",
        "\n",
        "#create model paramters with initial values \n",
        "W = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "\n",
        "#training info\n",
        "train_steps = 2500\n",
        "learning_rate = 0.001\n",
        "\n",
        "for i in range(train_steps):\n",
        "  \n",
        "  #watch the gradient flow \n",
        "  with tf.GradientTape() as tape:\n",
        "    \n",
        "    #forward pass \n",
        "    yhat = X * W + b\n",
        "    \n",
        "    #calcuate the loss (difference squared error)\n",
        "    error = yhat - y\n",
        "    loss = tf.reduce_mean(tf.square(error))\n",
        "  \n",
        "  #evalute the gradient with the respect to the paramters\n",
        "  dW, db = tape.gradient(loss, [W, b])\n",
        "\n",
        "  #update the paramters using Gradient Descent  \n",
        "  W.assign_sub(dW * learning_rate)\n",
        "  b.assign_sub(db* learning_rate)\n",
        "\n",
        "  #print the loss every 20 iterations \n",
        "  if i % 500 == 0:\n",
        "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss))\n",
        "      \n",
        "print(f'W : {W.numpy()} , b  = {b.numpy()} ')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at step 000: 14.065\n",
            "Loss at step 500: 2.817\n",
            "Loss at step 1000: 1.313\n",
            "Loss at step 1500: 1.112\n",
            "Loss at step 2000: 1.085\n",
            "W : 2.9625463485717773 , b  = 1.993432641029358 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqXDBIpEVzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "f642342e-3dd2-4984-cc48-cad7e6b31a55"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X, y, 'bo',label='org')\n",
        "plt.plot(X, y * W.numpy() + b.numpy(), 'r',\n",
        "         label=\"huber regression\")\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngLFqmAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy73ewUiZ09A",
        "colab_type": "text"
      },
      "source": [
        "# Things to Do[Homework Part 1             Point = 3]\n",
        "NOTE:- All submissions should use NIPS latex template [https://nips.cc/Conferences/2019/PaperInformation/StyleFiles].\n",
        "Pdf generated from NIPS template would only be accepted rest all would lead to zero points.\n",
        "\n",
        "  * Fork repo [https://github.com/AnkurMali/IST597_Fall2019_TF2.0] and modify file lin_reg.py.\n",
        "  * Change the loss function,which loss function works better and why?Write mathematical formulation for each loss function\n",
        "  * Create hybrid loss function(For eg. L1 + L2)\n",
        "  * Change the learning rate.\n",
        "  * Use patience scheduling[Whenever loss do not change , divide the learning rate by half].\n",
        "  * Train for longer Duration.\n",
        "  * Change the initial value for W and B.What effect it has on end result?\n",
        "  * Change the level of noise.\n",
        "  * Use various type of noise.\n",
        "  * Add noise in data.\n",
        "  * Add noise in your weights.\n",
        "  * Add noise in your learning rate[For all above: Scheme can be per  epoch or per N epochs]\n",
        "  * How do these changes effect the performance?\n",
        "  * Do you think these changes will have the same effect (if any) on other classification problems and mathematical models?\n",
        "  * Plot the different result.\n",
        "  * Do you get the exact same results if you run the Notebook multiple times without changing any parameters? Why or why not?[Explain significance of seed].\n",
        "  * Use unique seed for each experiment[Note:- Convert your first name into decimal].\n",
        "  * Later report per epoch GPU vs CPU Time.\n",
        "  * Can you get an model which is robust to noise?Does model lead to faster convergence?Do you get better local minima?Is noise beneficial?  \n",
        "  * Collect everything and report your findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBk2spWDu6h1",
        "colab_type": "text"
      },
      "source": [
        "#Part-2 Logistic Regression [Point =5 ]\n",
        "\n",
        "Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n",
        "In this you will be using Fashion Mnist which is a dataset of Zalando's article imagesconsisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classe[https://github.com/zalandoresearch/fashion-mnist].FashionMnist is much harder than MNIST so getting accuracy in 90's is difficult. You can use whatever loss functions, optimizers, even models that you want, as long as your model is built in TensorFlow using eager execution[Remember no keras is allowed]\n",
        "\n",
        "#Things to Report\n",
        "\n",
        "* Fork repo [https://github.com/AnkurMali/IST597_Fall2019_TF2.0] and modify file log_reg.py.\n",
        "* TODO: This keyword means you have to implement specific section/function/formula.\n",
        "* Report should contain matplotlib plots from function plot_images and plot_weights.\n",
        "* Change the optimizer and report which one converges faster and which one reaches better local minima/generalizes better.[Now you can use tensorflow optimizer]\n",
        "* Train for longer epochs.\n",
        "* Change Train/Val split.Report if you observe any performance drop/gain.\n",
        "* Report Train/Val accuracy over time.\n",
        "* Does batch size have any effect on performance.\n",
        "* Report GPU vs CPU per epoch performance.\n",
        "* Do Model overfit?If so why and also report measures you took to avoid overfitting.\n",
        "* Bonus points[Cluster the weights using any clustering mechanism].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFjw6c7H_u1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}