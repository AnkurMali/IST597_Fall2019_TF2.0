{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST597_mlp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt6zvw6zd2YO",
        "colab_type": "text"
      },
      "source": [
        "#IST597- softmax model(First step towards building your neural network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_qqH0TCCbx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68LhmrMICdic",
        "colab_type": "text"
      },
      "source": [
        "Load tensorflow and mnist data_loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVwpJUmfCpds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfe.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2krCqRcCq65",
        "colab_type": "text"
      },
      "source": [
        "Check whether or not you're working in eager execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1AKNWVkCzuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = tf.get_variable(name=\"W\", shape=(784, 10))\n",
        "b = tf.get_variable(name=\"b\", shape=(10, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZaI6Xy3C16c",
        "colab_type": "text"
      },
      "source": [
        "Create your tensorflow variables\n",
        "Create Weight and Biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIg-qgZSDIT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_model(image_batch):\n",
        "    model_output = tf.nn.softmax(tf.matmul(image_batch, W) + b)\n",
        "    return model_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlEOU3mdDKr8",
        "colab_type": "text"
      },
      "source": [
        "We have created the softmax_model\n",
        "output = F(X.W+b)\n",
        "where X is input , W is weight and b is biases.\n",
        "F is non-linear function , softmax in our case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S09GtsH3D241",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy(model_output, label_batch):\n",
        "    loss = tf.reduce_mean(\n",
        "        -tf.reduce_sum(label_batch * tf.log(model_output),\n",
        "        reduction_indices=[1]))\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mezQBqoeELOr",
        "colab_type": "text"
      },
      "source": [
        "Define your loss: In this case we would be using cross-entropy(NLL)\n",
        "Cross-entropy loss:- log loss is responsible for measuring the performance of a model which gives value between 0 and 1. A perfect model would have a log loss of 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMRl00yAGdpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tfe.implicit_value_and_gradients\n",
        "def cal_gradient(image_batch, label_batch):\n",
        "    return cross_entropy(softmax_model(image_batch), label_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJouUWYIGpwm",
        "colab_type": "text"
      },
      "source": [
        "This would returns a function which differentiates loss function with respect to variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tdA5t6W_Mb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e75101c-7952-4fa4-aa03-8c723114122d"
      },
      "source": [
        "lr=0.01\n",
        "batch_size=64\n",
        "epoch_n=10\n",
        "data = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
        "           .shuffle(buffer_size=1000)\\\n",
        "           .batch(batch_size=64)\\\n",
        "\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "\n",
        "for epoch in range(epoch_n):\n",
        "  for step, (image_batch, label_batch) in enumerate(tfe.Iterator(train_ds)):\n",
        "      loss, grads_and_vars = cal_gradient(image_batch, label_batch)\n",
        "      optimizer.apply_gradients(grads_and_vars)\n",
        "      if(step%100 == 0):\n",
        "        print(\"step: {}  loss: {}\".format(step, loss.numpy()))\n",
        "\n",
        "model_test_output = softmax_model(data.test.images)\n",
        "model_test_label = data.test.labels\n",
        "correct_prediction = tf.equal(tf.argmax(model_test_output, 1), tf.argmax(model_test_label, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "print(\"test accuracy = {}\".format(accuracy.numpy()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step: 0  loss: 2.4309253692626953\n",
            "step: 100  loss: 1.6556241512298584\n",
            "step: 200  loss: 1.2155452966690063\n",
            "step: 300  loss: 1.0475245714187622\n",
            "step: 400  loss: 1.0862095355987549\n",
            "step: 500  loss: 0.7784752249717712\n",
            "step: 600  loss: 0.8664791584014893\n",
            "step: 700  loss: 0.7498409748077393\n",
            "step: 800  loss: 0.698955774307251\n",
            "step: 0  loss: 0.6215534210205078\n",
            "step: 100  loss: 0.643366813659668\n",
            "step: 200  loss: 0.5203976631164551\n",
            "step: 300  loss: 0.6068369150161743\n",
            "step: 400  loss: 0.7616375088691711\n",
            "step: 500  loss: 0.5042001008987427\n",
            "step: 600  loss: 0.6564253568649292\n",
            "step: 700  loss: 0.5584914088249207\n",
            "step: 800  loss: 0.5750884413719177\n",
            "step: 0  loss: 0.47606679797172546\n",
            "step: 100  loss: 0.5176450610160828\n",
            "step: 200  loss: 0.4088672995567322\n",
            "step: 300  loss: 0.5149017572402954\n",
            "step: 400  loss: 0.6724463105201721\n",
            "step: 500  loss: 0.4298582673072815\n",
            "step: 600  loss: 0.584412693977356\n",
            "step: 700  loss: 0.4898722171783447\n",
            "step: 800  loss: 0.5216456651687622\n",
            "step: 0  loss: 0.4172459840774536\n",
            "step: 100  loss: 0.46674051880836487\n",
            "step: 200  loss: 0.3576718270778656\n",
            "step: 300  loss: 0.47118520736694336\n",
            "step: 400  loss: 0.6281046271324158\n",
            "step: 500  loss: 0.39349788427352905\n",
            "step: 600  loss: 0.5446990728378296\n",
            "step: 700  loss: 0.45289987325668335\n",
            "step: 800  loss: 0.48751479387283325\n",
            "step: 0  loss: 0.38431769609451294\n",
            "step: 100  loss: 0.43784481287002563\n",
            "step: 200  loss: 0.3266371488571167\n",
            "step: 300  loss: 0.44390714168548584\n",
            "step: 400  loss: 0.6007729768753052\n",
            "step: 500  loss: 0.3711305856704712\n",
            "step: 600  loss: 0.5183553099632263\n",
            "step: 700  loss: 0.42935237288475037\n",
            "step: 800  loss: 0.4623306393623352\n",
            "step: 0  loss: 0.36290255188941956\n",
            "step: 100  loss: 0.4184116721153259\n",
            "step: 200  loss: 0.30524325370788574\n",
            "step: 300  loss: 0.42441099882125854\n",
            "step: 400  loss: 0.581855058670044\n",
            "step: 500  loss: 0.3556104302406311\n",
            "step: 600  loss: 0.4991150200366974\n",
            "step: 700  loss: 0.4128991961479187\n",
            "step: 800  loss: 0.4423584043979645\n",
            "step: 0  loss: 0.34770071506500244\n",
            "step: 100  loss: 0.4039914906024933\n",
            "step: 200  loss: 0.28935688734054565\n",
            "step: 300  loss: 0.40934592485427856\n",
            "step: 400  loss: 0.5677635669708252\n",
            "step: 500  loss: 0.3440224528312683\n",
            "step: 600  loss: 0.4842134714126587\n",
            "step: 700  loss: 0.4006926417350769\n",
            "step: 800  loss: 0.4258308708667755\n",
            "step: 0  loss: 0.33626943826675415\n",
            "step: 100  loss: 0.39260417222976685\n",
            "step: 200  loss: 0.27697357535362244\n",
            "step: 300  loss: 0.39712393283843994\n",
            "step: 400  loss: 0.5567216873168945\n",
            "step: 500  loss: 0.3349366784095764\n",
            "step: 600  loss: 0.4722058176994324\n",
            "step: 700  loss: 0.3912419080734253\n",
            "step: 800  loss: 0.4117637276649475\n",
            "step: 0  loss: 0.3273158669471741\n",
            "step: 100  loss: 0.38322532176971436\n",
            "step: 200  loss: 0.26698529720306396\n",
            "step: 300  loss: 0.3868834376335144\n",
            "step: 400  loss: 0.547744631767273\n",
            "step: 500  loss: 0.32756030559539795\n",
            "step: 600  loss: 0.4622482657432556\n",
            "step: 700  loss: 0.38368332386016846\n",
            "step: 800  loss: 0.3995473384857178\n",
            "step: 0  loss: 0.32008713483810425\n",
            "step: 100  loss: 0.37526506185531616\n",
            "step: 200  loss: 0.25872135162353516\n",
            "step: 300  loss: 0.3781081438064575\n",
            "step: 400  loss: 0.5402401685714722\n",
            "step: 500  loss: 0.3214142322540283\n",
            "step: 600  loss: 0.4538081884384155\n",
            "step: 700  loss: 0.37748056650161743\n",
            "step: 800  loss: 0.3887752890586853\n",
            "test accuracy = 0.90829998254776\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}